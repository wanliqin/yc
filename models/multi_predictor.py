"""
å¤šç»´åº¦é¢„æµ‹å™¨ - æ‰©å±•é¢„æµ‹èƒ½åŠ›
æ”¯æŒæ¶¨è·Œæ–¹å‘ã€æ¶¨è·Œå¹…åº¦ã€æ³¢åŠ¨ç‡é¢„æµ‹
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error
import lightgbm as lgb
import xgboost as xgb
from models.stock_predictor import StockPredictor
import optuna

class MultiDimensionalPredictor(StockPredictor):
    """å¤šç»´åº¦è‚¡ç¥¨é¢„æµ‹å™¨"""
    
    def __init__(self, feature_columns: List[str]):
        super().__init__(feature_columns)
        self.prediction_types = ['direction', 'magnitude', 'volatility']
        self.models = {}
        
    def prepare_labels(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """å‡†å¤‡å¤šç»´åº¦æ ‡ç­¾
        
        Args:
            df: åŒ…å«ä»·æ ¼æ•°æ®çš„DataFrame
            
        Returns:
            DictåŒ…å«ä¸‰ç§é¢„æµ‹æ ‡ç­¾ï¼š
            - direction: æ¶¨è·Œæ–¹å‘ (0/1)
            - magnitude: æ¶¨è·Œå¹…åº¦ (%)
            - volatility: æœªæ¥æ³¢åŠ¨ç‡
        """
        labels = {}
        
        # 1. æ¶¨è·Œæ–¹å‘æ ‡ç­¾ (åŸæœ‰é€»è¾‘)
        labels['direction'] = (df['close'].shift(-1) > df['close']).astype(int)
        
        # 2. æ¶¨è·Œå¹…åº¦æ ‡ç­¾ (è¿ç»­å€¼)
        labels['magnitude'] = ((df['close'].shift(-1) - df['close']) / df['close'] * 100)
        
        # 3. æ³¢åŠ¨ç‡æ ‡ç­¾ (æœªæ¥5æ—¥æ ‡å‡†å·®)
        future_returns = df['close'].pct_change().shift(-5).rolling(window=5).std() * 100
        labels['volatility'] = future_returns
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        for key in labels:
            labels[key] = labels[key].fillna(labels[key].mean())
            
        return labels
    
    def train_multi_models(self, X: pd.DataFrame, labels: Dict[str, pd.Series],
                          window_size: int = 30, trials: int = 20) -> Dict[str, Any]:
        """è®­ç»ƒå¤šç»´åº¦æ¨¡å‹
        
        Args:
            X: ç‰¹å¾æ•°æ®
            labels: å¤šç»´åº¦æ ‡ç­¾å­—å…¸
            window_size: æ—¶é—´çª—å£å¤§å°
            trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        results = {}
        
        for pred_type, y in labels.items():
            print(f"ğŸ”„ è®­ç»ƒ{pred_type}é¢„æµ‹æ¨¡å‹...")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X_clean = X.dropna()
            y_clean = y.loc[X_clean.index]
            
            if len(X_clean) < window_size + 2:
                print(f"âŒ {pred_type}æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ")
                continue
                
            # æ ¹æ®é¢„æµ‹ç±»å‹é€‰æ‹©æ¨¡å‹
            if pred_type == 'direction':
                # åˆ†ç±»ä»»åŠ¡
                model, params, score = self._train_classification_model(
                    X_clean, y_clean, window_size, trials
                )
                metric_name = "å‡†ç¡®ç‡"
            else:
                # å›å½’ä»»åŠ¡
                model, params, score = self._train_regression_model(
                    X_clean, y_clean, window_size, trials
                )
                metric_name = "RÂ²åˆ†æ•°"
            
            results[pred_type] = {
                'model': model,
                'params': params,
                'score': score,
                'metric': metric_name,
                'data_size': len(X_clean)
            }
            
            print(f"âœ… {pred_type}æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œ{metric_name}: {score:.4f}")
            
        return results
    
    def _train_classification_model(self, X: pd.DataFrame, y: pd.Series,
                                  window_size: int, trials: int) -> Tuple[Any, Dict, float]:
        """è®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼ˆæ¶¨è·Œæ–¹å‘ï¼‰"""
        
        def objective(trial):
            model_type = trial.suggest_categorical('model', ['lgb', 'xgb', 'rf'])
            
            if model_type == 'lgb':
                params = {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 10, 300),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                    'verbose': -1
                }
                
                # æ—¶åºäº¤å‰éªŒè¯
                scores = []
                for i in range(window_size, len(X) - 1, 10):
                    X_train = X.iloc[max(0, i-window_size):i]
                    y_train = y.iloc[max(0, i-window_size):i]
                    X_val = X.iloc[i:i+1]
                    y_val = y.iloc[i:i+1]
                    
                    if len(X_train) < 10 or len(X_val) == 0:
                        continue
                        
                    train_data = lgb.Dataset(X_train, label=y_train)
                    model = lgb.train(params, train_data, num_boost_round=100)
                    pred = (model.predict(X_val) > 0.5).astype(int)
                    scores.append(accuracy_score(y_val, pred))
                
                return np.mean(scores) if scores else 0
                
            elif model_type == 'xgb':
                params = {
                    'objective': 'binary:logistic',
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                }
                
                scores = []
                for i in range(window_size, len(X) - 1, 10):
                    X_train = X.iloc[max(0, i-window_size):i]
                    y_train = y.iloc[max(0, i-window_size):i]
                    X_val = X.iloc[i:i+1]
                    y_val = y.iloc[i:i+1]
                    
                    if len(X_train) < 10 or len(X_val) == 0:
                        continue
                        
                    model = xgb.XGBClassifier(**params, random_state=42)
                    model.fit(X_train, y_train)
                    pred = model.predict(X_val)
                    scores.append(accuracy_score(y_val, pred))
                
                return np.mean(scores) if scores else 0
                
            else:  # rf
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                    'max_depth': trial.suggest_int('max_depth', 3, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                    'random_state': 42
                }
                
                scores = []
                for i in range(window_size, len(X) - 1, 10):
                    X_train = X.iloc[max(0, i-window_size):i]
                    y_train = y.iloc[max(0, i-window_size):i]
                    X_val = X.iloc[i:i+1]
                    y_val = y.iloc[i:i+1]
                    
                    if len(X_train) < 10 or len(X_val) == 0:
                        continue
                        
                    model = RandomForestClassifier(**params)
                    model.fit(X_train, y_train)
                    pred = model.predict(X_val)
                    scores.append(accuracy_score(y_val, pred))
                
                return np.mean(scores) if scores else 0
        
        # ä¼˜åŒ–è¶…å‚æ•°
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=trials, show_progress_bar=False)
        
        best_params = study.best_params
        model_type = best_params.pop('model')
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        if model_type == 'lgb':
            best_params.update({
                'objective': 'binary',
                'metric': 'binary_logloss',
                'verbose': -1
            })
            train_data = lgb.Dataset(X, label=y)
            final_model = lgb.train(best_params, train_data, num_boost_round=100, verbose_eval=False)
        elif model_type == 'xgb':
            final_model = xgb.XGBClassifier(**best_params, random_state=42)
            final_model.fit(X, y)
        else:
            final_model = RandomForestClassifier(**best_params)
            final_model.fit(X, y)
            
        return final_model, best_params, study.best_value
    
    def _train_regression_model(self, X: pd.DataFrame, y: pd.Series,
                              window_size: int, trials: int) -> Tuple[Any, Dict, float]:
        """è®­ç»ƒå›å½’æ¨¡å‹ï¼ˆå¹…åº¦/æ³¢åŠ¨ç‡ï¼‰"""
        
        def objective(trial):
            model_type = trial.suggest_categorical('model', ['lgb', 'xgb', 'rf'])
            
            if model_type == 'lgb':
                params = {
                    'objective': 'regression',
                    'metric': 'rmse',
                    'boosting_type': 'gbdt',
                    'num_leaves': trial.suggest_int('num_leaves', 10, 300),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                    'verbose': -1
                }
                
                scores = []
                for i in range(window_size, len(X) - 1, 10):
                    X_train = X.iloc[max(0, i-window_size):i]
                    y_train = y.iloc[max(0, i-window_size):i]
                    X_val = X.iloc[i:i+1]
                    y_val = y.iloc[i:i+1]
                    
                    if len(X_train) < 10 or len(X_val) == 0:
                        continue
                        
                    train_data = lgb.Dataset(X_train, label=y_train)
                    model = lgb.train(params, train_data, num_boost_round=100, verbose_eval=False)
                    pred = model.predict(X_val)
                    
                    # RÂ²åˆ†æ•°
                    ss_res = np.sum((y_val - pred) ** 2)
                    ss_tot = np.sum((y_val - y_val.mean()) ** 2)
                    r2 = 1 - (ss_res / (ss_tot + 1e-8))
                    scores.append(r2)
                
                return np.mean(scores) if scores else 0
                
            elif model_type == 'xgb':
                params = {
                    'objective': 'reg:squarederror',
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                }
                
                scores = []
                for i in range(window_size, len(X) - 1, 10):
                    X_train = X.iloc[max(0, i-window_size):i]
                    y_train = y.iloc[max(0, i-window_size):i]
                    X_val = X.iloc[i:i+1]
                    y_val = y.iloc[i:i+1]
                    
                    if len(X_train) < 10 or len(X_val) == 0:
                        continue
                        
                    model = xgb.XGBRegressor(**params, random_state=42)
                    model.fit(X_train, y_train)
                    pred = model.predict(X_val)
                    
                    ss_res = np.sum((y_val - pred) ** 2)
                    ss_tot = np.sum((y_val - y_val.mean()) ** 2)
                    r2 = 1 - (ss_res / (ss_tot + 1e-8))
                    scores.append(r2)
                
                return np.mean(scores) if scores else 0
                
            else:  # rf
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                    'max_depth': trial.suggest_int('max_depth', 3, 20),
                    'random_state': 42
                }
                
                scores = []
                for i in range(window_size, len(X) - 1, 10):
                    X_train = X.iloc[max(0, i-window_size):i]
                    y_train = y.iloc[max(0, i-window_size):i]
                    X_val = X.iloc[i:i+1]
                    y_val = y.iloc[i:i+1]
                    
                    if len(X_train) < 10 or len(X_val) == 0:
                        continue
                        
                    model = RandomForestRegressor(**params)
                    model.fit(X_train, y_train)
                    pred = model.predict(X_val)
                    
                    ss_res = np.sum((y_val - pred) ** 2)
                    ss_tot = np.sum((y_val - y_val.mean()) ** 2)
                    r2 = 1 - (ss_res / (ss_tot + 1e-8))
                    scores.append(r2)
                
                return np.mean(scores) if scores else 0
        
        # ä¼˜åŒ–è¶…å‚æ•°
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=trials, show_progress_bar=False)
        
        best_params = study.best_params
        model_type = best_params.pop('model')
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        if model_type == 'lgb':
            best_params.update({
                'objective': 'regression',
                'metric': 'rmse',
                'verbose': -1
            })
            train_data = lgb.Dataset(X, label=y)
            final_model = lgb.train(best_params, train_data, num_boost_round=100, verbose_eval=False)
        elif model_type == 'xgb':
            final_model = xgb.XGBRegressor(**best_params, random_state=42)
            final_model.fit(X, y)
        else:
            final_model = RandomForestRegressor(**best_params)
            final_model.fit(X, y)
            
        return final_model, best_params, study.best_value
    
    def predict_multi_dimensions(self, X: pd.DataFrame, models: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """å¤šç»´åº¦é¢„æµ‹
        
        Args:
            X: ç‰¹å¾æ•°æ®
            models: è®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        predictions = {}
        
        for pred_type, model_info in models.items():
            model = model_info['model']
            
            if pred_type == 'direction':
                # åˆ†ç±»é¢„æµ‹
                if hasattr(model, 'predict_proba'):
                    pred_proba = model.predict_proba(X)[:, 1]
                    predictions[pred_type] = (pred_proba > 0.5).astype(int)
                    predictions[f'{pred_type}_prob'] = pred_proba
                else:  # LightGBM
                    pred_proba = model.predict(X)
                    predictions[pred_type] = (pred_proba > 0.5).astype(int)
                    predictions[f'{pred_type}_prob'] = pred_proba
            else:
                # å›å½’é¢„æµ‹
                predictions[pred_type] = model.predict(X)
        
        return predictions
    
    def get_comprehensive_prediction(self, X: pd.DataFrame, models: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–ç»¼åˆé¢„æµ‹ç»“æœ
        
        Args:
            X: ç‰¹å¾æ•°æ®ï¼ˆå•è¡Œï¼‰
            models: è®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸
            
        Returns:
            ç»¼åˆé¢„æµ‹ç»“æœ
        """
        predictions = self.predict_multi_dimensions(X, models)
        
        # æ„å»ºç»¼åˆç»“æœ
        result = {
            'direction': {
                'prediction': int(predictions['direction'][0]),
                'probability': float(predictions['direction_prob'][0]),
                'signal': 'ä¸Šæ¶¨' if predictions['direction'][0] == 1 else 'ä¸‹è·Œ'
            },
            'magnitude': {
                'prediction': float(predictions['magnitude'][0]),
                'description': f"é¢„æœŸæ¶¨è·Œå¹…: {predictions['magnitude'][0]:.2f}%"
            },
            'volatility': {
                'prediction': float(predictions['volatility'][0]),
                'description': f"é¢„æœŸæ³¢åŠ¨ç‡: {predictions['volatility'][0]:.2f}%"
            }
        }
        
        # é£é™©ç­‰çº§è¯„ä¼°
        volatility = predictions['volatility'][0]
        if volatility < 1.0:
            risk_level = "ä½é£é™©"
        elif volatility < 2.5:
            risk_level = "ä¸­ç­‰é£é™©"
        else:
            risk_level = "é«˜é£é™©"
            
        result['risk_assessment'] = {
            'level': risk_level,
            'volatility': volatility
        }
        
        return result
